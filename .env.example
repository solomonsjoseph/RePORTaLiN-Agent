# =============================================================================
# RePORTaLiN MCP Server Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values.
# NEVER commit .env to version control!
# =============================================================================

# -----------------------------------------------------------------------------
# Environment & Logging
# -----------------------------------------------------------------------------
# Deployment environment: local, development, staging, production
# - local/development: Pretty logs, relaxed validation, dev token auto-gen
# - staging/production: JSON logs, strict validation, requires auth token
ENVIRONMENT=local

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log format: auto, json, pretty
# - auto: Pretty for local/dev, JSON for staging/prod (recommended)
# - json: Always output structured JSON logs
# - pretty: Always output colored, human-readable logs
LOG_FORMAT=auto

# -----------------------------------------------------------------------------
# Server Configuration
# -----------------------------------------------------------------------------
# Server bind host (use 0.0.0.0 for external access)
MCP_HOST=127.0.0.1

# Server port for HTTP/SSE transport
MCP_PORT=8000

# Transport protocol: stdio, http, sse
# - stdio: Standard input/output (for Claude Desktop, CLI)
# - http: HTTP REST API
# - sse: Server-Sent Events (for streaming responses)
MCP_TRANSPORT=stdio

# -----------------------------------------------------------------------------
# Security Configuration (CRITICAL)
# -----------------------------------------------------------------------------
# Authentication token for API access
# Generate a secure token with: python -c "import secrets; print(secrets.token_hex(32))"
# REQUIRED in production/staging environments!
MCP_AUTH_TOKEN=

# Enable/disable authentication (should always be true in production)
# Only set to false when server is behind an authenticating proxy
MCP_AUTH_ENABLED=true

# CORS allowed origins (comma-separated list)
# Set to specific domains in production, empty = none allowed
# Example: https://app.example.com,https://admin.example.com
CORS_ALLOWED_ORIGINS=

# -----------------------------------------------------------------------------
# Privacy Configuration
# -----------------------------------------------------------------------------
# Privacy enforcement mode: strict, standard
# - strict: Enforces DPDPA/HIPAA compliance (REQUIRED in production)
# - standard: Relaxed validation for development/testing
PRIVACY_MODE=strict

# Minimum k-anonymity threshold for aggregate statistics
# Must be >= 1, typically 5-10 for production
MIN_K_ANONYMITY=5

# -----------------------------------------------------------------------------
# Data Paths
# -----------------------------------------------------------------------------
# Path to data dictionary JSONL files (relative to project root)
DATA_DICTIONARY_PATH=results/data_dictionary_mappings

# Path to encrypted audit logs (relative to project root)
ENCRYPTED_LOGS_PATH=encrypted_logs

# -----------------------------------------------------------------------------
# LLM Provider API Keys (Optional - for enhanced features)
# -----------------------------------------------------------------------------
# OpenAI API Key (for embeddings/completion if used)
OPENAI_API_KEY=

# Anthropic API Key (for Claude integration if used)
ANTHROPIC_API_KEY=

# -----------------------------------------------------------------------------
# Agent Configuration (for client/agent.py)
# -----------------------------------------------------------------------------
# LLM API Key (defaults to OPENAI_API_KEY if not set)
# Required unless using a local LLM with LLM_BASE_URL
LLM_API_KEY=

# LLM Base URL - enables local LLM support (Ollama, LM Studio, etc.)
# Leave empty for OpenAI API, or set to local endpoint:
# - Ollama: http://localhost:11434/v1
# - LM Studio: http://localhost:1234/v1
# - vLLM: http://localhost:8000/v1
LLM_BASE_URL=

# LLM Model to use
# - OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
# - Ollama: llama3.2, mistral, codellama, etc.
LLM_MODEL=gpt-4o-mini

# MCP Server URL for the agent to connect to
MCP_SERVER_URL=http://localhost:8000/mcp/sse

# Agent behavior settings
AGENT_MAX_ITERATIONS=10
AGENT_TEMPERATURE=0.7

# -----------------------------------------------------------------------------
# Encryption (for audit logs)
# -----------------------------------------------------------------------------
# Fernet encryption key for audit logs
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
REPORTALIN_ENCRYPTION_KEY=

# -----------------------------------------------------------------------------
# Development Settings (ignored in production)
# -----------------------------------------------------------------------------
# Enable development mode features (verbose logging, auto-reload, etc.)
# DEV_MODE=1

# Enable debug endpoints (health check details, config dump, etc.)
# DEBUG_ENDPOINTS=1
